{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of FIW-model2 VGG16.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKg1dYIReo9k",
        "colab_type": "code",
        "outputId": "bc2f724f-b686-46ba-e3c0-e309a72b125d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive',force_remount = True)\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "from keras import callbacks\n",
        "import keras.backend as K\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import os\n",
        "import pandas as pd\n",
        "import ast\n",
        "import random\n",
        "import keras\n",
        "import math\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from sklearn import metrics\n",
        "%matplotlib inline\n",
        "from keras.optimizers import SGD,Adam\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.applications.resnet50 import preprocess_input\n",
        "from keras.models import Sequential,Model\n",
        "from keras.models import model_from_json\n",
        "from keras.layers import Dense,ZeroPadding2D,Convolution2D,MaxPooling2D,Dropout,Flatten,Activation,Input,concatenate\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "%cd /gdrive\n",
        "# Most Recent One\n",
        "!pip install git+https://github.com/rcmalli/keras-vggface.git\n",
        "# Release Version\n",
        "!pip install keras_vggface\n",
        "from keras_vggface.vggface import VGGFace"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/gdrive\n",
            "Collecting git+https://github.com/rcmalli/keras-vggface.git\n",
            "  Cloning https://github.com/rcmalli/keras-vggface.git to /tmp/pip-req-build-tkgyxe7g\n",
            "  Running command git clone -q https://github.com/rcmalli/keras-vggface.git /tmp/pip-req-build-tkgyxe7g\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.5) (1.16.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.5) (1.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.5) (2.8.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.5) (4.3.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.5) (2.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.5) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.5) (3.13)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->keras-vggface==0.5) (0.46)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-vggface==0.5) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-vggface==0.5) (1.0.8)\n",
            "Building wheels for collected packages: keras-vggface\n",
            "  Building wheel for keras-vggface (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2c4jbkdf/wheels/36/07/46/06c25ce8e9cd396dabe151ea1d8a2bc28dafcb11321c1f3a6d\n",
            "Successfully built keras-vggface\n",
            "Installing collected packages: keras-vggface\n",
            "Successfully installed keras-vggface-0.5\n",
            "Requirement already satisfied: keras_vggface in /usr/local/lib/python3.6/dist-packages (0.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (1.12.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (2.2.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (4.3.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (1.16.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (1.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras_vggface) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras_vggface) (1.0.8)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->keras_vggface) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy2cLHs8Vv0H",
        "colab_type": "text"
      },
      "source": [
        "**Making the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oKWH59gXRWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/gdrive/My Drive/data-kaggle/train/'\n",
        "k = 0\n",
        "farr = []\n",
        "for fid in os.listdir(path):\n",
        "  marr = []\n",
        "  marr.append(k)\n",
        "  marr.append(fid)\n",
        "  path1 = os.path.join(path,fid)\n",
        "  for mid in os.listdir(path1):\n",
        "    t = []\n",
        "    t.append(mid)\n",
        "    path2 = os.path.join(path1,mid)\n",
        "    if (os.listdir(path2) == []):\n",
        "      continue\n",
        "    for pid in os.listdir(path2):\n",
        "      t.append(pid)\n",
        "    marr.append(t)\n",
        "  farr.append(marr)\n",
        "  k +=1\n",
        "posanc = []\n",
        "#imgp1 = path1 + '/' + farr[j+2]\n",
        "for i in range(len(farr)):\n",
        "  pathf = path + farr[i][1] + '/'\n",
        "  l = len(farr[i])-2\n",
        "  t2 = []\n",
        "  t3 = []\n",
        "  for j in range(l):\n",
        "    pathm1 = pathf + farr[i][j+2][0] + '/'\n",
        "    l1  = len(farr[i][j+2])-1\n",
        "    for c in range(j+1,l):\n",
        "      l2 = len(farr[i][c+2])-1\n",
        "      pathm2 = pathf + farr[i][c+2][0] + '/'\n",
        "      for k in range(l1):\n",
        "        pathp1 = pathm1 + farr[i][j+2][k+1]\n",
        "        for g in range(l2):\n",
        "          pathp2 = pathm2 + farr[i][c+2][g+1]\n",
        "          temp = []\n",
        "          temp.append(pathp1)\n",
        "          temp.append(pathp2)\n",
        "          t2.append(temp)\n",
        "          del(temp)\n",
        "  t3.append(i)\n",
        "  t3.append(farr[i][1])\n",
        "  t3.append(t2)\n",
        "  posanc.append(t3)\n",
        "  del(t3)\n",
        "  del(t2)\n",
        "s =0\n",
        "for i in range(len(posanc)):\n",
        "  s += len(posanc[i][2])\n",
        "import random\n",
        "posancneg = []\n",
        "l = len(posanc)\n",
        "for i in range(l):\n",
        "  for j in range(len(posanc[i][2])):\n",
        "    if(i<l/2):\n",
        "      c = random.randint(i+1,l-1)\n",
        "    else:\n",
        "      c = random.randint(0,i-1)\n",
        "    if (len(farr[c])-1)==1:\n",
        "      f =c;\n",
        "      break;\n",
        "      f1 = m;\n",
        "    m = random.randint(2,len(farr[c])-1) \n",
        "    if len(farr[c][m])-1==0:\n",
        "      f =c;\n",
        "      f1 = m;\n",
        "      break;\n",
        "    p = random.randint(1,len(farr[c][m])-1)\n",
        "    path3 = path + farr[c][1] + '/' + farr[c][m][0] + '/' + farr[c][m][p]\n",
        "    posanc[i][2][j].append(path3)\n",
        "    \n",
        "    \n",
        "    \n",
        "train_all=[]\n",
        "for i in range(len(posanc)):\n",
        "  for j in range(len(posanc[i][2])):\n",
        "    train_all.append(posanc[i][2][j])\n",
        "\n",
        "\n",
        "file=open('/gdrive/My Drive/data-kaggle/outputHard.txt','r')\n",
        "if file.mode == 'r':\n",
        "  contents =file.read()\n",
        "images=ast.literal_eval(contents)\n",
        "    \n",
        "len_val=200\n",
        "\n",
        "\n",
        "train=[]\n",
        "for j in random.sample(range(0,410182), 200000):\n",
        "  train.append(train_all[j])\n",
        "\n",
        " \n",
        "\n",
        "train_anchor=[]\n",
        "train_positive=[]\n",
        "train_negative=[]\n",
        "for i in range(len(train)):\n",
        "  train_anchor.append(train[i][0])\n",
        "  train_positive.append(train[i][1])\n",
        "  train_negative.append(train[i][2])\n",
        "  \n",
        "\n",
        "validation=[]\n",
        "    \n",
        "for j in random.sample(range(0,410182), 3000):\n",
        "  validation.append(train_all[j])\n",
        "\n",
        "    \n",
        "\n",
        "val_set=[]\n",
        "for i in range(len(validation)):\n",
        "  a=random.randint(1,2)\n",
        "  b=[]\n",
        "  b.append(validation[i][0])\n",
        "  \n",
        "  if a==1:\n",
        "    b.append(validation[i][1])\n",
        "    b.append(1)\n",
        "  else:\n",
        "    b.append(validation[i][2])\n",
        "    b.append(0)\n",
        "  val_set.append(b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEuLN0WmjPFw",
        "colab_type": "text"
      },
      "source": [
        "**Image Alignment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgMdRB1Sephh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_base_network(in_dims):\n",
        "  nb_class = 128\n",
        "  hidden_dim = 512\n",
        "\n",
        "  vgg_model = VGGFace(weights='vggface',include_top=True, input_shape=(224, 224, 3))\n",
        "  last_layer = vgg_model.get_layer('fc6/relu').output\n",
        "\n",
        "\n",
        "  out = Dense(nb_class, activation='tanh', name='fc8')(x)\n",
        "  custom_vgg_model = Model(vgg_model.input, out)\n",
        "  \n",
        "  for layer in custom_vgg_model.layers[:-6]:\n",
        "    layer.trainable = False\n",
        "      \n",
        "\n",
        "  return custom_vgg_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlTcwONp8TWi",
        "colab_type": "code",
        "outputId": "21a448ac-ea4e-4b29-b051-05ecf6766e89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        }
      },
      "source": [
        "model=create_base_network((224,224,3))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E0704 12:50:21.414760 139627899881344 ultratb.py:152] Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-16-a0cdceaaaec8>\", line 1, in <module>\n",
            "    model=create_base_network((224,224,3))\n",
            "  File \"<ipython-input-15-9a0ed95efeeb>\", line 9, in create_base_network\n",
            "    out = Dense(nb_class, activation='tanh', name='fc8')(x)\n",
            "NameError: name 'x' is not defined\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 725, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 709, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.6/posixpath.py\", line 383, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEZ-pmWgnSn8",
        "colab_type": "text"
      },
      "source": [
        "**Triplet Loss Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1IBTVB2SsoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "def triplet_loss(y_true,y_pred, alpha = 0.4):\n",
        "    total_lenght = y_pred.shape.as_list()[-1]\n",
        "\n",
        "    anchor = y_pred[:,0:int(total_lenght*1/3)]\n",
        "    positive = y_pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
        "    negative = y_pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
        "\n",
        "    # distance between the anchor and the positive\n",
        "    pos_dist = K.sum(K.square(anchor-positive),axis=1)\n",
        "\n",
        "    # distance between the anchor and the negative\n",
        "    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n",
        "\n",
        "    # compute loss\n",
        "    basic_loss = pos_dist-neg_dist+alpha\n",
        "    loss = K.maximum(basic_loss,0.0)\n",
        " \n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu0-21cvVERG",
        "colab_type": "code",
        "outputId": "e7352ad8-d580-4550-e6fd-94be4ef47e2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "adam_optim = Adam(lr=0.01, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "\n",
        "anchor_input = Input((224,224, 3), name='anchor_input')\n",
        "positive_input = Input((224,224, 3), name='positive_input')\n",
        "negative_input = Input((224,224, 3), name='negative_input')\n",
        "\n",
        "Shared_DNN = create_base_network((224,224,3))\n",
        "\n",
        "encoded_anchor = Shared_DNN(anchor_input)\n",
        "encoded_positive = Shared_DNN(positive_input)\n",
        "encoded_negative = Shared_DNN(negative_input)\n",
        "\n",
        "merged_vector = concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=-1, name='merged_layer')\n",
        "print(merged_vector)\n",
        "model = Model(inputs=[anchor_input,positive_input, negative_input], outputs=merged_vector)\n",
        "model.summary()\n",
        "model.compile(loss=triplet_loss, optimizer=adam_optim)\n",
        "\n",
        "def someMethodToLoadImages(files,batch_start,limit):\n",
        "  arr=[]\n",
        "  for i in range(batch_start,limit):\n",
        "    img=cv2.imread(files[i],0)\n",
        "    img=np.array(img)\n",
        "    img=np.resize(img  ,(224,224,3))\n",
        "    arr.append(img)\n",
        "  return arr\n",
        "\n",
        "def imageLoader(train_positive,train_negative, train_anchor, batch_size=64):\n",
        "    \n",
        "    L = len(train_positive)\n",
        "    while True:\n",
        "      batch_start = 0\n",
        "      batch_end = batch_size\n",
        "      X=[]\n",
        "      print(batch_start)\n",
        "      while batch_start < L:\n",
        "        limit = min(batch_end, L)\n",
        "        A = preprocess_input(np.array(someMethodToLoadImages(train_anchor,batch_start,limit)))\n",
        "        P = preprocess_input(np.array(someMethodToLoadImages(train_positive,batch_start,limit)))\n",
        "        N = preprocess_input(np.array(someMethodToLoadImages(train_negative,batch_start,limit)))\n",
        "        \n",
        "        X=[A,P,N]\n",
        "        Y = np.empty((np.array(A).shape[0],3072))\n",
        "        Y = np.array(Y)\n",
        "        yield (X,Y) \n",
        "        batch_start += batch_size   \n",
        "        batch_end += batch_size\n",
        "              "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"merged_layer_6/concat:0\", shape=(?, 384), dtype=float32)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "anchor_input (InputLayer)       (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "positive_input (InputLayer)     (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "negative_input (InputLayer)     (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_13 (Model)                (None, 128)          27625920    anchor_input[0][0]               \n",
            "                                                                 positive_input[0][0]             \n",
            "                                                                 negative_input[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "merged_layer (Concatenate)      (None, 384)          0           model_13[1][0]                   \n",
            "                                                                 model_13[2][0]                   \n",
            "                                                                 model_13[3][0]                   \n",
            "==================================================================================================\n",
            "Total params: 27,625,920\n",
            "Trainable params: 12,911,232\n",
            "Non-trainable params: 14,714,688\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFNm2cn1cXEW",
        "colab_type": "text"
      },
      "source": [
        "#CALLBACKS\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ij9Jvp8f7SD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_prob(loaded_model, img1,img2):\n",
        "  dummy=np.zeros((1,224,224,3))\n",
        "  img1=preprocess_input(np.resize((np.array(cv2.imread(img1,0))),(1,224,224,3)))\n",
        "  img2=preprocess_input(np.resize((np.array(cv2.imread(img2,0))),(1,224,224,3)))\n",
        "  s=loaded_model.predict([img2, img1, dummy])\n",
        "\n",
        "  s1=(s[0][128:256]).reshape(1,-1)\n",
        "\n",
        "  s2=(s[0][0:128]).reshape(1,-1)\n",
        " \n",
        "  similarity=metrics.pairwise.cosine_similarity(s1,s2)\n",
        "  similarity=float(similarity)\n",
        "  return similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89WdSD3MOX1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha = 0.1\n",
        "roc = []\n",
        "class roc_test(callbacks.Callback):\n",
        "  def on_epoch_end(self, epochs, logs={}):\n",
        "     if (epochs+1)%10 == 0:\n",
        "       prob=[]\n",
        "       true=[]\n",
        "       for i in range(200):\n",
        "         similarity=predict_prob(model, val_set[i][0],val_set[i][1])\n",
        "         prob.append(similarity)\n",
        "         true.append(val_set[i][2])\n",
        "       roc.append(roc_auc_score(true, prob))\n",
        "       l = len(roc)\n",
        "       print(\"current roc score: \",roc[l-1])\n",
        "       if(l>=2):\n",
        "         if(roc[l-1]<roc[l-2]):\n",
        "           lr = K.get_value(adam_optim.lr)\n",
        "           K.set_value(adam_optim.lr, alpha * lr)\n",
        "           print(\"learning rate:\",lr,\"new rate:\",alpha*lr)\n",
        "         elif(roc[l-1]>roc[l-2]): \n",
        "           print('roc improved .. saving...')\n",
        "           filepath =  '/gdrive/My Drive/weights/' + str(epochs+1) \n",
        "           model_json = model.to_json()\n",
        "           with open(filepath + '.json', 'w') as json_file:\n",
        "            json_file.write(model_json)\n",
        "           # serialize weights to HDF5\n",
        "           model.save_weights(filepath + '.h5')\n",
        "           print(\"Saved \", str(epochs+1),\" epoch to disk\")\n",
        "          \n",
        "          \n",
        "roctest = roc_test()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx70EKNlOuAY",
        "colab_type": "code",
        "outputId": "a0d27394-7bd5-4aae-b0f6-5d037809c308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size=32\n",
        "model.fit_generator(imageLoader(train_positive,train_negative, train_anchor,batch_size),steps_per_epoch=5, epochs=2000, verbose=1, callbacks = [roctest])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20000\n",
            "\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.4525\n",
            "Epoch 2/2000\n",
            "5/5 [==============================] - 5s 940ms/step - loss: 0.4450\n",
            "Epoch 3/2000\n",
            "5/5 [==============================] - 5s 951ms/step - loss: 0.4425\n",
            "Epoch 4/2000\n",
            "5/5 [==============================] - 5s 945ms/step - loss: 0.3975\n",
            "Epoch 5/2000\n",
            "5/5 [==============================] - 5s 953ms/step - loss: 0.4225\n",
            "Epoch 6/2000\n",
            "5/5 [==============================] - 5s 948ms/step - loss: 0.4150\n",
            "Epoch 7/2000\n",
            "5/5 [==============================] - 5s 948ms/step - loss: 0.4650\n",
            "Epoch 8/2000\n",
            "5/5 [==============================] - 5s 954ms/step - loss: 0.4200\n",
            "Epoch 9/2000\n",
            "5/5 [==============================] - 5s 954ms/step - loss: 0.4450\n",
            "Epoch 10/2000\n",
            "5/5 [==============================] - 5s 917ms/step - loss: 0.4225\n",
            "current roc score:  0.4956931089743589\n",
            "Epoch 11/2000\n",
            "5/5 [==============================] - 5s 914ms/step - loss: 0.4225\n",
            "Epoch 12/2000\n",
            "5/5 [==============================] - 64s 13s/step - loss: 0.4175\n",
            "Epoch 13/2000\n",
            "5/5 [==============================] - 117s 23s/step - loss: 0.4284\n",
            "Epoch 14/2000\n",
            "5/5 [==============================] - 96s 19s/step - loss: 0.4450\n",
            "Epoch 15/2000\n",
            "5/5 [==============================] - 98s 20s/step - loss: 0.8075\n",
            "Epoch 16/2000\n",
            "5/5 [==============================] - 121s 24s/step - loss: 0.4175\n",
            "Epoch 17/2000\n",
            "5/5 [==============================] - 102s 20s/step - loss: 0.4150\n",
            "Epoch 18/2000\n",
            "5/5 [==============================] - 92s 18s/step - loss: 0.3875\n",
            "Epoch 19/2000\n",
            "5/5 [==============================] - 85s 17s/step - loss: 0.4575\n",
            "Epoch 20/2000\n",
            "5/5 [==============================] - 72s 14s/step - loss: 0.8675\n",
            "current roc score:  0.501201923076923\n",
            "roc improved .. saving...\n",
            "Saved  20  epoch to disk\n",
            "Epoch 21/2000\n",
            "5/5 [==============================] - 58s 12s/step - loss: 0.2898\n",
            "Epoch 22/2000\n",
            "5/5 [==============================] - 87s 17s/step - loss: 0.4250\n",
            "Epoch 23/2000\n",
            "5/5 [==============================] - 87s 17s/step - loss: 0.3975\n",
            "Epoch 24/2000\n",
            "5/5 [==============================] - 74s 15s/step - loss: 0.4250\n",
            "Epoch 25/2000\n",
            "5/5 [==============================] - 17s 3s/step - loss: 0.3675\n",
            "Epoch 26/2000\n",
            "5/5 [==============================] - 22s 4s/step - loss: 0.4000\n",
            "Epoch 27/2000\n",
            "5/5 [==============================] - 84s 17s/step - loss: 0.4475\n",
            "Epoch 28/2000\n",
            "5/5 [==============================] - 71s 14s/step - loss: 0.3975\n",
            "Epoch 29/2000\n",
            "5/5 [==============================] - 69s 14s/step - loss: 0.4000\n",
            "Epoch 30/2000\n",
            "5/5 [==============================] - 65s 13s/step - loss: 0.3975\n",
            "current roc score:  0.501201923076923\n",
            "Epoch 31/2000\n",
            "5/5 [==============================] - 42s 8s/step - loss: 0.3975\n",
            "Epoch 32/2000\n",
            "5/5 [==============================] - 63s 13s/step - loss: 0.3975\n",
            "Epoch 33/2000\n",
            "5/5 [==============================] - 69s 14s/step - loss: 0.4000\n",
            "Epoch 34/2000\n",
            "5/5 [==============================] - 65s 13s/step - loss: 0.3950\n",
            "Epoch 35/2000\n",
            "5/5 [==============================] - 59s 12s/step - loss: 0.3975\n",
            "Epoch 36/2000\n",
            "5/5 [==============================] - 66s 13s/step - loss: 0.4450\n",
            "Epoch 37/2000\n",
            "5/5 [==============================] - 63s 13s/step - loss: 0.4250\n",
            "Epoch 38/2000\n",
            "5/5 [==============================] - 56s 11s/step - loss: 0.3975\n",
            "Epoch 39/2000\n",
            "5/5 [==============================] - 58s 12s/step - loss: 0.4000\n",
            "Epoch 40/2000\n",
            "5/5 [==============================] - 52s 10s/step - loss: 0.4225\n",
            "current roc score:  0.501201923076923\n",
            "Epoch 41/2000\n",
            "5/5 [==============================] - 27s 5s/step - loss: 0.4225\n",
            "Epoch 42/2000\n",
            "5/5 [==============================] - 53s 11s/step - loss: 0.3975\n",
            "Epoch 43/2000\n",
            "2/5 [===========>..................] - ETA: 26s - loss: 0.4000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E0627 11:08:35.666271 139658024834944 ultratb.py:152] Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-38-102d514858f5>\", line 2, in <module>\n",
            "    model.fit_generator(imageLoader(train_positive,train_negative, train_anchor,batch_size),steps_per_epoch=5, epochs=2000, verbose=1, callbacks = [roctest])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1418, in fit_generator\n",
            "    initial_epoch=initial_epoch)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\", line 181, in fit_generator\n",
            "    generator_output = next(output_generator)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 685, in get\n",
            "    inputs = self.queue.get(block=True).get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 638, in get\n",
            "    self.wait(timeout)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 635, in wait\n",
            "    self._event.wait(timeout)\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 551, in wait\n",
            "    signaled = self._cond.wait(timeout)\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 295, in wait\n",
            "    waiter.acquire()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 725, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 709, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.6/posixpath.py\", line 383, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIEL5h05nDcl",
        "colab_type": "text"
      },
      "source": [
        "**Function to validate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvlIae6MLt80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title validation function\n",
        "def validate(model,val_set):\n",
        "  c=0\n",
        "  prob=[]\n",
        "  true=[]\n",
        "  for i in range(5):\n",
        "    print(i, end=',')\n",
        "    similarity=predict_prob(model, val_set[i][0],val_set[i][1])\n",
        "    prob.append(similarity)\n",
        "    true.append(val_set[i][2])\n",
        "  roc = roc_auc_score(true, prob)\n",
        "  print(roc)\n",
        "  return roc\n",
        "#validate(loaded_model, val_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yGRbwff52qR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load json and create model\n",
        "json_file = open('/gdrive/My Drive/weights/V2/12.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"/gdrive/My Drive/weights/V2/12.h5\")\n",
        "print(\"Loaded model from disk\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH6iYmpr6KVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaded_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}